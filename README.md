# MRI-to-CT

The dataset is preprocessed using the "dataset_creation.ipynb" notebook. Only coronal axis slice images were chosen for further analysis. This script generates image slices (transversal, sagittal, and coronal) from brain MRI scans. It reads MRI scans stored in NIfTI format, extracts slices at midpoints along each axis, and saves them as PNG files. The saved slices include transversal (T), sagittal (S), and coronal (C) views. The script utilizes libraries such as nibabel, nilearn, scipy, matplotlib, and tqdm for processing and visualization.

Canny images for the MRI images can be created using the "canny.py". This script utilizes OpenCV and PIL libraries to perform Canny edge detection. It reads images from a specified input directory, applies Canny edge detection with customizable thresholds, and saves the processed images to an output directory. This is necessary inorder to build the controlnet model.

For training the standard diffusin model, "sd_1_5_training_ddim.py" has been deployed. The code loads a dataset called coronal dataset for training with tokenization. It defines various training configurations such as image size, batch sizes, number of epochs, learning rate, etc. After defining the training configurations, it prepares data for training by resizing, converting to tensor, and normalization. The script initializes a UNet2D model for image generation and sets up a scheduler for adding noise during training. Finally optimizer and learning rate scheduler for model training is defined and the training loop is executed, which includes forward diffusion process, loss calculation, backward diffusion process, optimization, and logging. After each epoch, the script evaluates the model and saves samples during training epochs.

The "evaluation.py" script, facilitates the evaluation of the trained model for image generation using backward diffusion.

For the ControlNet, the main structure of the setup is derived from the "https://github.com/lllyasviel/ControlNet". But, in order to train the control in our setting, we altered some of the codes present in the original ControlNet github.

The "tutorial_dataset.py" script is designed to serve as a tutorial dataset generator. It provides functionalities to generate synthetic datasets with customizable parameters such as size, features, and labels. It also includes basic visualization functions to help understand the generated datasets.

The "tool_add_control.py" script serves to preprocess and adapt a pretrained model. The script loads a configuration file for the model and the pretrained model itself from the input path. Through a series of operations, it adapts the model's state dictionary to align with the current model structure, handling any discrepancies in node names. Upon completion, the adapted model is saved to the specified output path. Throughout the process, the script provides informative output, such as notifying about any weights that are newly added. This script essentially streamlines the process of preparing a pretrained model for use in a specific context or application.

The "tutorial_train.py" script trains a neural network model using PyTorch Lightning framework. It loads a pre-trained model checkpoint and sets up configurations such as batch size, logger frequency, and learning rate. The script creates the model architecture defined in 'cldm.model' module and loads its state dictionary. It sets additional model parameters like learning rate, sd_locked, and only_mid_control.
A custom dataset 'MyDataset' is used for training. The DataLoader is set up with specified batch size and shuffling. Finally, PyTorch Lightning Trainer is initialized with GPU support and other configurations, and the model is trained on the dataset.

To run the this after cloning with the before mentioned github repo and placing the dataset inside the repo:
#### 1. run python tutorial_dataset.py
#### 2. run python tool_add_control.py path/to/sd/model paht/to/control/model
#### 3. run python tutorial_train.py

Under the "cldm" directory, there are 5 other python scripts "cldm.py", "ddim_hacked.py", "hack.py", "logger.py", and "model.py" which are essential for the ControlNet model.

The "cldm.py" file contains a collection of classes essential for implementing a controlled diffusion model. The essential one is the ControlledUnetModel class, an extension of a UNet architecture, with mechanisms for incorporating control inputs during its forward pass. Complementing this model is the ControlNet class, which serves as a preprocessing module dedicated to refining control inputs before they interact with the main model. Finally, the ControlLDM class extends a Latent Diffusion model, integrating control mechanisms into the diffusion process. This class performs various functionalities crucial for model operation, including data input processing, sampling, image logging, and optimizer configuration. The important thing is that it offers a feature for dynamically adjusting memory usage based on whether the model is actively diffusing or not. Collectively, these classes constructs the controlled diffusion model, capable of generating or denoising images while responsive to diverse control inputs.

The "ddim_hacked.py" module provides functionality for sampling from a diffusion model using a modified method known as DDIM (Diffusion-Decomposition Implicit Model). The DDIMSampler class initializes the sampling process, setting up the necessary parameters and schedules. It offers methods like sample, ddim_sampling, encode, stochastic_encode, and decode for various sampling tasks. The sample method generates samples from the diffusion model, while encode encodes an input image into its latent representation. Conversely, decode decodes a latent representation back into an image. These methods support options for conditioning, dynamic thresholding, and handling noise. Additionally, the module allows for stochastic encoding and provides functionalities for modifying scores during sampling.

The "hack.py" script provides functions to manipulate and enhance certain functionalities within the codebase. The disable_verbosity() function sets the logging verbosity to error level, enable_sliced_attention() function modifies attention mechanisms, and hack_everything(clip_skip=0) enables hacks related to encoding text using the CLIP model. The _hacked_clip_forward() function modifies the CLIP embedder to handle input tokens differently, and _hacked_sliced_attentin_forward() function adjusts the attention mechanism by splitting computations into chunks for efficiency.

The logger.py file has a special tool called ImageLogger, made for PyTorch Lightning. It helps in logging images while training. This tool is set up to save pictures every so often, usually based on how many batches you've processed. It can adjust the brightness and number of pictures it saves. When you're training your model, it jumps in at the end of each batch and saves some images for you. One can turn this feature on or off whenever you like.

The "model.py" script provides essential functions for managing model checkpoints and creating models based on configuration files. The get_state_dict(d) function retrieves the state dictionary from a provided dictionary, while load_state_dict(ckpt_path, location='cpu') loads the model's state dictionary from a checkpoint file, supporting both ".safetensors" (in our case, the sd model had .safetensors extension) and regular PyTorch checkpoint files. Finally, the create_model(config_path) function instantiates a model based on the configuration file path provided.
